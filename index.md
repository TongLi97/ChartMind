<div style="text-align: center; max-width: 800px; margin: 0 auto;">

# ChartMind <img src="/images/chartMind.png" width="32" height="32" style="vertical-align: middle;"> : Benchmark and Reasoning Insights of Multimodal Chart Question Answering

**Tong Li, Guodao Sun, Shunkai Wang, Zuoyu Tang, Yang Shu, Xueqian Zheng, Haixia Wang, Ronghua Liang**

<!-- ÊåâÈíÆÊ†∑ÂºèÁöÑÈìæÊé• -->
<div style="text-align: center; margin: 30px 0;">
  <a href="#" style="background-color: #2174A8; color: white; padding: 8px 16px; margin: 0 8px; text-decoration: none; border-radius: 5px; display: inline-block;">üìë Paper</a>
  <a href="#" style="background-color: #2174A8; color: white; padding: 8px 16px; margin: 0 8px; text-decoration: none; border-radius: 5px; display: inline-block;">‚≠ê Code</a>
  <a href="https://huggingface.co/datasets/guodaosun/Mega60k" style="background-color: #2174A8; color: white; padding: 8px 16px; margin: 0 8px; text-decoration: none; border-radius: 5px; display: inline-block;">üß± Dataset</a>
</div>

<!-- ÊµÅÁ®ãÂõæ -->
<div style="text-align: center; margin: 40px 0;">
  <img src="/images/cover.png" alt="ChartMind Overview" style="max-width: 100%; height: auto;">
</div>

## Abstract
Existing ChartQA evaluations for multimodal large language models focuses on visual-only input, and rely solely on ``black-box'' accuracy metrics, offering limited insight into reasoning traces. To fill these gaps, we introduce Mega60k, a benchmark covering 21 chart types and 11 QA tasks; collect ChartQA reasoning traces from MLLMs; and propose the reasoning deconstruction framework to parse multimodal activation patterns and reasoning evidence usage. Evaluating 12 representative MLLMs (7 open-source and 5 closed-source) under three conditions‚Äîvisual-only, multimodal fusion, and multimodal compensation‚Äîreveals key findings: High-level tasks (multi-step logic, visual pattern recognition, layout optimization) serve as gold-standard for distinguishing MLLMs; Mere modality stacking struggles to extend reasoning boundaries but shows compensatory potential in quantitative visual understanding tasks; Gemini 2.5 Flash and GPT-4o demonstrate positive signals in leveraging structured modality reasoning to mitigate visual degradation such as omissions, occlusion, blurring, and rotation.

## Overview
[Longer description of the project, methodology, and key contributions]

## Key Features
- Feature 1
- Feature 2  
- Feature 3

## Results
[Brief overview of main findings/results]

## Resources
- **Paper**: [Link to paper]
- **Code**: [GitHub repository]
- **Dataset**: [Link to data]

## Citation

</div>
